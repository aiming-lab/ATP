defaults:
  - train: grpo
  - reward: atp_sa
  - dataset: atp_sa_llm
  - _self_

run-name: atp-sa-llm-grpo-qwen3-8B
cuda_visible_devices: '0,1,2,3,4,5,6,7'
model_id: "Qwen/Qwen3-8B"

train:
  run_name: ${run-name}
  output_dir: training_output/${run-name}
  logging_steps: 1
  max_prompt_length: 1024
  report_to: wandb
  wandb_name: ${train.run_name}
  wandb_project: ATP
  save_steps: 50
  num_generations: 4
  bf16: True
  attn_implementation: flash_attention_2 # comment this line if not using flash attention
  max_completion_length: 2048
  num_train_epochs: 6.0
  learning_rate: 1e-06
  gradient_accumulation_steps: 4
  deepspeed: config/deepspeed/zero3.json # comment this line if not using deepspeed